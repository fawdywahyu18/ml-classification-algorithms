{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: A1 group-set1 177\n",
    "##### Student 1 SID: 540660818\n",
    "##### Student 2 SID: 550247340  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"rice-final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "for col in df.columns[:-1]:  \n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1])\n",
    "scaler = MinMaxScaler()\n",
    "df.iloc[:, :-1] = scaler.fit_transform(df.iloc[:, :-1])\n",
    "df.iloc[:, -1] = df.iloc[:, -1].replace({\"class1\": 0, \"class2\": 1}).astype(int)\n",
    "X = df.iloc[:, :-1].to_numpy() # Fitur\n",
    "y = df.iloc[:, -1].to_numpy() # Class variable\n",
    "def print_data(X, y, n_rows=10):\n",
    "    for example_num in range(n_rows):\n",
    "        print(\",\".join(\"{:.4f}\".format(feature) for feature in X[example_num]), end=\",\")\n",
    "        print(y[example_num])\n",
    "\n",
    "print_data(X, y, n_rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "y = np.array(y, dtype=int)\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def dtClassifier(X, y):\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Bagging with Decision Trees\n",
    "def bagDTClassifier(X, y, n_estimators=50, max_samples=1, max_depth=None):\n",
    "    base_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = BaggingClassifier(base_tree, n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# AdaBoost with Decision Trees\n",
    "def adaDTClassifier(X, y, n_estimators=50, learning_rate=1, max_depth=1):\n",
    "    base_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = AdaBoostClassifier(base_tree, n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Gradient Boosting\n",
    "def gbClassifier(X, y, n_estimators=50, learning_rate=0.1):\n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR average cross-validation accuracy: 0.9386\n",
      "NB average cross-validation accuracy: 0.9264\n",
      "DT average cross-validation accuracy: 0.9179\n",
      "Bagging average cross-validation accuracy: 0.9400\n",
      "AdaBoost average cross-validation accuracy: 0.9407\n",
      "GB average cross-validation accuracy: 0.9321\n"
     ]
    }
   ],
   "source": [
    "# Running the classifiers and printing the results for Part 1\n",
    "\n",
    "# Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "# AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_max_depth = 5\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "logreg_score = logregClassifier(X, y)\n",
    "nb_score = nbClassifier(X, y)\n",
    "dt_score = dtClassifier(X, y)\n",
    "bag_score = bagDTClassifier(X, y, n_estimators=bag_n_estimators, max_samples=0.8, max_depth=bag_max_depth)\n",
    "ada_score = adaDTClassifier(X, y, n_estimators=ada_n_estimators, learning_rate=ada_learning_rate, max_depth=ada_max_depth)\n",
    "gb_score = gbClassifier(X, y, n_estimators=gb_n_estimators, learning_rate=gb_learning_rate)\n",
    "\n",
    "print(f\"LR average cross-validation accuracy: {logreg_score:.4f}\")\n",
    "print(f\"NB average cross-validation accuracy: {nb_score:.4f}\")\n",
    "print(f\"DT average cross-validation accuracy: {dt_score:.4f}\")\n",
    "print(f\"Bagging average cross-validation accuracy: {bag_score:.4f}\")\n",
    "print(f\"AdaBoost average cross-validation accuracy: {ada_score:.4f}\")\n",
    "print(f\"GB average cross-validation accuracy: {gb_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_k = grid_search.best_params_['n_neighbors']\n",
    "    best_p = grid_search.best_params_['p']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred) \n",
    "    return best_k, best_p, best_cv_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    svm = SVC(kernel='rbf', random_state=0)\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    best_gamma = grid_search.best_params_['gamma']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    return best_C, best_gamma, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    rf = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    best_max_leaf_nodes = grid_search.best_params_['max_leaf_nodes']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    test_weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_n_estimators, best_max_leaf_nodes, best_cv_accuracy, test_accuracy, test_macro_f1, test_weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 5\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9371\n",
      "KNN test set accuracy: 0.9257\n",
      "\n",
      "SVM best C: 5\n",
      "SVM best gamma: 1\n",
      "SVM cross-validation accuracy: 0.9457\n",
      "SVM test set accuracy: 0.9343\n",
      "\n",
      "RF best n_estimators: 30\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.9390\n",
      "RF test set accuracy: 0.9371\n",
      "RF test set macro average F1: 0.9355\n",
      "RF test set weighted average F1: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "best_k, best_p, knn_cv_accuracy, knn_test_accuracy = bestKNNClassifier(X, y)\n",
    "print(\"KNN best k:\", best_k)\n",
    "print(\"KNN best p:\", best_p)\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn_cv_accuracy))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn_test_accuracy))\n",
    "print()\n",
    "\n",
    "# SVM\n",
    "best_C, best_gamma, svm_cv_accuracy, svm_test_accuracy = bestSVMClassifier(X, y)\n",
    "print(\"SVM best C:\", best_C)\n",
    "print(\"SVM best gamma:\", best_gamma)\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(svm_cv_accuracy))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(svm_test_accuracy))\n",
    "print()\n",
    "\n",
    "# Random Forest\n",
    "best_n_estimators, best_max_leaf_nodes, rf_cv_accuracy, rf_test_accuracy, rf_macro_f1, rf_weighted_f1 = bestRFClassifier(X, y)\n",
    "print(\"RF best n_estimators:\", best_n_estimators)\n",
    "print(\"RF best max_leaf_nodes:\", best_max_leaf_nodes)\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf_cv_accuracy))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf_test_accuracy))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(rf_macro_f1))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(rf_weighted_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### Student 1: Throughout this assignment, the most important thing I learned was the significance of model selection, hyperparameter tuning, and evaluation techniques in machine learning. By comparing different classifiers, I observed how each algorithm performs differently on the same dataset. This assignment underscored that no single model is universally best.\n",
    "##### Student 2: The results are better if ensemble methods are used. This is because ensembles combine base classifiers and take the average results to increase accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
