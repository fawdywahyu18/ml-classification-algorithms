{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: ...\n",
    "##### Student 1 SID: 540660818\n",
    "##### Student 2 SID: ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Area               1400 non-null   object\n",
      " 1   Perimiter          1400 non-null   object\n",
      " 2   Major_Axis_Length  1400 non-null   object\n",
      " 3   Minor_Axis_Length  1400 non-null   object\n",
      " 4   Eccentricity       1400 non-null   object\n",
      " 5   Convex_Area        1400 non-null   object\n",
      " 6   Extent             1400 non-null   object\n",
      " 7   class              1400 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 87.6+ KB\n",
      "None\n",
      "\n",
      "Descriptive Statistics:\n",
      "         Area Perimiter Major_Axis_Length Minor_Axis_Length Eccentricity  \\\n",
      "count    1400      1400              1400              1400         1400   \n",
      "unique   1259      1389              1396              1397         1393   \n",
      "top     12837         ?                 ?                 ?            ?   \n",
      "freq        4         4                 5                 3            6   \n",
      "\n",
      "       Convex_Area Extent   class  \n",
      "count         1400   1400    1400  \n",
      "unique        1256   1399       2  \n",
      "top              ?      ?  class2  \n",
      "freq             5      2     800  \n",
      "\n",
      "First few rows of the dataset:\n",
      "    Area    Perimiter Major_Axis_Length Minor_Axis_Length Eccentricity  \\\n",
      "0  12573  461.4660034       192.9033508       84.57207489  0.898771763   \n",
      "1  12845  464.1210022       194.3322144       85.52433777  0.897951961   \n",
      "2  14055  488.7489929       207.7517548       87.25032806  0.907536149   \n",
      "3  14412  490.3240051       207.4761353       89.68951416  0.901735425   \n",
      "4  14658  477.1170044       189.5666351       99.99777985  0.849550545   \n",
      "\n",
      "  Convex_Area       Extent   class  \n",
      "0       12893  0.550433397  class2  \n",
      "1       13125  0.774962306  class2  \n",
      "2       14484  0.550076306  class1  \n",
      "3       14703  0.598853171  class1  \n",
      "4       15048  0.649503708  class2  \n",
      "\n",
      "Missing values per column:\n",
      "Area                 0\n",
      "Perimiter            0\n",
      "Major_Axis_Length    0\n",
      "Minor_Axis_Length    0\n",
      "Eccentricity         0\n",
      "Convex_Area          0\n",
      "Extent               0\n",
      "class                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"rice-final2.csv\"  # Adjust the path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1\n"
     ]
    }
   ],
   "source": [
    "# Pre-process dataset\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Replace \"?\" with NaN\n",
    "df.replace(\"?\", np.nan, inplace=True)\n",
    "\n",
    "# Convert all feature columns to float\n",
    "for col in df.columns[:-1]:  \n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to float, coercing errors to NaN\n",
    "\n",
    "# Fill missing values with column mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df.iloc[:, :-1] = imputer.fit_transform(df.iloc[:, :-1])  # Apply only to feature columns\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df.iloc[:, :-1] = scaler.fit_transform(df.iloc[:, :-1])\n",
    "\n",
    "# Convert class labels: \"class1\" -> 0, \"class2\" -> 1\n",
    "df.iloc[:, -1] = df.iloc[:, -1].replace({\"class1\": 0, \"class2\": 1}).astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy arrays\n",
    "X = df.iloc[:, :-1].to_numpy()  # Features\n",
    "y = df.iloc[:, -1].to_numpy()   # Class labels\n",
    "\n",
    "# Define the modified print_data function\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first n_rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: number of rows to print (default is 10)\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        # Print feature values formatted to 4 decimal places\n",
    "        print(\",\".join(\"{:.4f}\".format(feature) for feature in X[example_num]), end=\",\")\n",
    "        # Print class label without decimal places\n",
    "        print(y[example_num])\n",
    "\n",
    "# Call print_data function with X and y\n",
    "print_data(X, y, n_rows=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Ensure y is an integer array\n",
    "y = np.array(y, dtype=int)  # Convert y to integer type\n",
    "\n",
    "# Define StratifiedKFold with 10 splits and random_state=0\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Function for Logistic Regression Classifier\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression(random_state=0, max_iter=1000)  # Ensure convergence with more iterations\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Function for Na√Øve Bayes Classifier\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Function for Decision Tree Classifier\n",
    "def dtClassifier(X, y):\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)  # Using Information Gain (Entropy)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Function for Bagging with Decision Trees\n",
    "def bagDTClassifier(X, y, n_estimators=50, max_samples=1.0, max_depth=None):\n",
    "    base_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = BaggingClassifier(base_tree, n_estimators=n_estimators, max_samples=max_samples, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Function for AdaBoost with Decision Trees\n",
    "def adaDTClassifier(X, y, n_estimators=50, learning_rate=1.0, max_depth=1):\n",
    "    base_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = AdaBoostClassifier(base_tree, n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "# Function for Gradient Boosting\n",
    "def gbClassifier(X, y, n_estimators=50, learning_rate=0.1):\n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9386\n",
      "NB average cross-validation accuracy: 0.9264\n",
      "DT average cross-validation accuracy: 0.9179\n",
      "Bagging average cross-validation accuracy: 0.9400\n",
      "AdaBoost average cross-validation accuracy: 0.9407\n",
      "GB average cross-validation accuracy: 0.9321\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "# Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "# AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_max_depth = 5\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Run classifiers and store results\n",
    "logreg_score = logregClassifier(X, y)\n",
    "nb_score = nbClassifier(X, y)\n",
    "dt_score = dtClassifier(X, y)\n",
    "bag_score = bagDTClassifier(X, y, n_estimators=bag_n_estimators, max_samples=0.8, max_depth=bag_max_depth)\n",
    "ada_score = adaDTClassifier(X, y, n_estimators=ada_n_estimators, learning_rate=ada_learning_rate, max_depth=ada_max_depth)\n",
    "gb_score = gbClassifier(X, y, n_estimators=gb_n_estimators, learning_rate=gb_learning_rate)\n",
    "\n",
    "# Print results for each classifier in Part 1 to 4 decimal places\n",
    "print(f\"LogR average cross-validation accuracy: {logreg_score:.4f}\")\n",
    "print(f\"NB average cross-validation accuracy: {nb_score:.4f}\")\n",
    "print(f\"DT average cross-validation accuracy: {dt_score:.4f}\")\n",
    "print(f\"Bagging average cross-validation accuracy: {bag_score:.4f}\")\n",
    "print(f\"AdaBoost average cross-validation accuracy: {ada_score:.4f}\")\n",
    "print(f\"GB average cross-validation accuracy: {gb_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Define the stratified KFold for cross-validation\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Finds the best KNN classifier using grid search with 10-fold stratified cross-validation.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_samples, n_features), feature matrix.\n",
    "        y: numpy array of shape (n_samples,), target labels.\n",
    "    \n",
    "    Returns:\n",
    "        best_k: Best value of k (n_neighbors).\n",
    "        best_p: Best value of p (distance metric).\n",
    "        best_cv_accuracy: Best cross-validation accuracy.\n",
    "        test_accuracy: Test set accuracy.\n",
    "    \"\"\"\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Define the parameter grid for KNN\n",
    "    param_grid = {'n_neighbors': k, 'p': p}\n",
    "    \n",
    "    # Initialize KNN classifier\n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    # Perform grid search with 10-fold stratified cross-validation\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters and cross-validation accuracy\n",
    "    best_k = grid_search.best_params_['n_neighbors']\n",
    "    best_p = grid_search.best_params_['p']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return best_k, best_p, best_cv_accuracy, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Finds the best SVM classifier using grid search with 10-fold stratified cross-validation.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_samples, n_features), feature matrix.\n",
    "        y: numpy array of shape (n_samples,), target labels.\n",
    "    \n",
    "    Returns:\n",
    "        best_C: Best value of C (regularization parameter).\n",
    "        best_gamma: Best value of gamma (kernel coefficient).\n",
    "        best_cv_accuracy: Best cross-validation accuracy.\n",
    "        test_accuracy: Test set accuracy.\n",
    "    \"\"\"\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Define the parameter grid for SVM\n",
    "    param_grid = {'C': C, 'gamma': gamma}\n",
    "    \n",
    "    # Initialize SVM classifier with RBF kernel\n",
    "    svm = SVC(kernel='rbf', random_state=0)\n",
    "    \n",
    "    # Perform grid search with 10-fold stratified cross-validation\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters and cross-validation accuracy\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    best_gamma = grid_search.best_params_['gamma']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return best_C, best_gamma, best_cv_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‚Äòsqrt‚Äô.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    \"\"\"\n",
    "    Finds the best Random Forest classifier using grid search with 10-fold stratified cross-validation.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_samples, n_features), feature matrix.\n",
    "        y: numpy array of shape (n_samples,), target labels.\n",
    "    \n",
    "    Returns:\n",
    "        best_n_estimators: Best value of n_estimators (number of trees).\n",
    "        best_max_leaf_nodes: Best value of max_leaf_nodes (maximum number of leaf nodes).\n",
    "        best_cv_accuracy: Best cross-validation accuracy.\n",
    "        test_accuracy: Test set accuracy.\n",
    "        test_macro_f1: Test set macro average F1 score.\n",
    "        test_weighted_f1: Test set weighted average F1 score.\n",
    "    \"\"\"\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # Define the parameter grid for Random Forest\n",
    "    param_grid = {'n_estimators': n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    \n",
    "    # Initialize Random Forest classifier with information gain and max_features='sqrt'\n",
    "    rf = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    \n",
    "    # Perform grid search with 10-fold stratified cross-validation\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=cvKFold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters and cross-validation accuracy\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    best_max_leaf_nodes = grid_search.best_params_['max_leaf_nodes']\n",
    "    best_cv_accuracy = grid_search.best_score_\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    test_weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_n_estimators, best_max_leaf_nodes, best_cv_accuracy, test_accuracy, test_macro_f1, test_weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 5\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9371\n",
      "KNN test set accuracy: 0.9257\n",
      "\n",
      "SVM best C: 5\n",
      "SVM best gamma: 1\n",
      "SVM cross-validation accuracy: 0.9457\n",
      "SVM test set accuracy: 0.9343\n",
      "\n",
      "RF best n_estimators: 30\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.9390\n",
      "RF test set accuracy: 0.9371\n",
      "RF test set macro average F1: 0.9355\n",
      "RF test set weighted average F1: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "\n",
    "# KNN\n",
    "best_k, best_p, knn_cv_accuracy, knn_test_accuracy = bestKNNClassifier(X, y)\n",
    "print(\"KNN best k:\", best_k)\n",
    "print(\"KNN best p:\", best_p)\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn_cv_accuracy))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn_test_accuracy))\n",
    "print()\n",
    "\n",
    "# SVM\n",
    "best_C, best_gamma, svm_cv_accuracy, svm_test_accuracy = bestSVMClassifier(X, y)\n",
    "print(\"SVM best C:\", best_C)\n",
    "print(\"SVM best gamma:\", best_gamma)\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(svm_cv_accuracy))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(svm_test_accuracy))\n",
    "print()\n",
    "\n",
    "# Random Forest\n",
    "best_n_estimators, best_max_leaf_nodes, rf_cv_accuracy, rf_test_accuracy, rf_macro_f1, rf_weighted_f1 = bestRFClassifier(X, y)\n",
    "print(\"RF best n_estimators:\", best_n_estimators)\n",
    "print(\"RF best max_leaf_nodes:\", best_max_leaf_nodes)\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf_cv_accuracy))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf_test_accuracy))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(rf_macro_f1))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(rf_weighted_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### Student 1: ...\n",
    "##### Student 2: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
